{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import model_selection, feature_extraction, preprocessing, svm, pipeline, metrics, tree, linear_model\n",
    "from project_utilities import import_train_data, import_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\35904\\Desktop\\Mcgill_Study\\551ML\\project2\\train\\train')\n",
    "path = os.getcwd()\n",
    "import_train_data = import_train_data(path)\n",
    "\n",
    "os.chdir(r'C:\\Users\\35904\\Desktop\\Mcgill_Study\\551ML\\project2\\test\\test')\n",
    "path = os.getcwd()\n",
    "import_test_data = import_test_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(import_train_data)\n",
    "test_data = pd.DataFrame(import_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good to use 80% and 20% of the dataset for training and testing respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = model_selection.train_test_split(\n",
    "    train_data.drop(columns=['category']), \n",
    "    train_data.drop(columns=['text']), \n",
    "    test_size=0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Different Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following experiments are already using pipelining. The pipeline structures the raw data. It also extracts and selects features from the structured data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipeline_logistic_reg = pipeline.Pipeline([\n",
    "    ('vect', feature_extraction.text.CountVectorizer()),\n",
    "    ('norm', preprocessing.Normalizer()),\n",
    "    ('clf', linear_model.LogisticRegression())])\n",
    "\n",
    "clf_pipeline_logistic_reg.fit(X_train['text'], y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg_predictions = clf_pipeline_logistic_reg.predict(X_validation['text'])\n",
    "\n",
    "print(metrics.classification_report(y_validation, logistic_reg_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipeline_tree = pipeline.Pipeline([\n",
    "    ('vect', feature_extraction.text.CountVectorizer()),\n",
    "    ('norm', preprocessing.Normalizer()),\n",
    "    ('clf', tree.DecisionTreeClassifier())])\n",
    "\n",
    "clf_pipeline_tree.fit(X_train['text'], y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_predictions = clf_pipeline_tree.predict(X_validation['text'])\n",
    "\n",
    "print(metrics.classification_report(y_validation, tree_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipeline_svm = pipeline.Pipeline([\n",
    "    ('vect', feature_extraction.text.CountVectorizer()),\n",
    "    ('norm', preprocessing.Normalizer()),\n",
    "    ('clf', svm.LinearSVC())])\n",
    "\n",
    "clf_pipeline_svm.fit(X_train['text'], y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictions = clf_pipeline_svm.predict(X_validation['text'])\n",
    "\n",
    "print(metrics.classification_report(y_validation, svm_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Different Feature Extraction Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipeline_svm_bin = pipeline.Pipeline([\n",
    "    ('vect', feature_extraction.text.HashingVectorizer(binary=True)),#Convert a collection of text documents to a matrix of token occurrences\n",
    "    ('norm', preprocessing.Normalizer()),\n",
    "    ('clf', svm.LinearSVC())])\n",
    "\n",
    "clf_pipeline_svm_bin.fit(X_train['text'], y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_bin_predictions = clf_pipeline_svm_bin.predict(X_validation['text'])\n",
    "\n",
    "print(metrics.classification_report(y_validation, svm_bin_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipeline_svm_tfidf = pipeline.Pipeline([\n",
    "    ('vect', feature_extraction.text.CountVectorizer(tokenizer = token.tokenize)), #(nltk.word_tokenize,textblob_tokenizer,token.tokenize is good)\n",
    "    ('tfidf', feature_extraction.text.TfidfTransformer()),\n",
    "    ('norm', preprocessing.Normalizer()),\n",
    "    ('clf', svm.LinearSVC())])\n",
    "\n",
    "clf_pipeline_svm_tfidf.fit(X_train['text'], y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_tfidf_predictions = clf_pipeline_svm_tfidf.predict(X_validation['text'])\n",
    "\n",
    "print(metrics.classification_report(y_validation, svm_tfidf_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some customer parameters\n",
    "These parameters will used in GridResearch and RandomResearch to see if they can improve the occurency\n",
    "\n",
    "```textblob_tokenizer```, ```stemming_tokenizer```, ```token.tokenize```, and ```nltk.word_tokenize``` is the customer paramerters for paremeter ```tokenize``` in ```CountVectorizer```, and ```MyAnalyzer``` is the customer parameters for ```analysis``` of ```CountVectorizer```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### textblob_tokenizer and stemming_tokenizer\n",
    "There two worse the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Use TextBlob\n",
    "def textblob_tokenizer(str_input):\n",
    "    blob = TextBlob(str_input.lower())\n",
    "    tokens = blob.words\n",
    "    words = [token.stem() for token in tokens]\n",
    "    return words\n",
    "\n",
    "# Use NLTK's PorterStemmer\n",
    "def stemming_tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### token.tokenize\n",
    "Improve the results which is the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#tokenizer to remove unwanted elements from out data like symbols and numbers\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk.word_tokenize\n",
    "Improve the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\35904\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyAnalyzer\n",
    "Worse the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAnalyzer(object):\n",
    "    \n",
    "    # load spaCy's english model and define the tokenizer/lemmatizer\n",
    "    def __init__(self):\n",
    "        spacy.load('en')\n",
    "        self.lemmatizer_ = spacy.lang.en.English()\n",
    "        \n",
    "    # allow the class instance to be called just like\n",
    "    # just like a function and applies the preprocessing and\n",
    "    # tokenize the document\n",
    "    def __call__(self, doc):\n",
    "        doc_clean = unescape(doc).lower()\n",
    "        tokens = self.lemmatizer_(doc_clean)\n",
    "        return([token.lemma_ for token in tokens])\n",
    "    \n",
    "analyzer = MyAnalyzer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF-IDF and Linear SVM with GridSearch Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our best classifier so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipeline_svm = pipeline.Pipeline([\n",
    "    ('vect', feature_extraction.text.CountVectorizer()),\n",
    "    ('tfidf', feature_extraction.text.TfidfTransformer()),\n",
    "    ('norm', preprocessing.Normalizer()),\n",
    "    ('clf', svm.LinearSVC())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NuSVC (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipeline_svm = pipeline.Pipeline([\n",
    "    ('vect', feature_extraction.text.CountVectorizer()),#tokenizer=nltk.word_tokenize\n",
    "    ('tfidf', feature_extraction.text.TfidfTransformer()),\n",
    "    ('norm', preprocessing.Normalizer()),\n",
    "    ('clf', svm.NuSVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipeline_svm.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1,2)],#(1,1), (2,2),(1,3),(2,3)\n",
    "              'vect__max_features':[None],  # max (20000, 68354) #,10000,20000, 30000,40000, 50000,60000\n",
    "              'vect__binary': [True],#  , False\n",
    "              'vect__strip_accents': ['ascii'], #,'unicode',None\n",
    "              'vect__analyzer':['word'],# 'char', 'char_wb'\n",
    "              'vect__max_df' :[1.0], # 'C': [0.1, 1, 10, 100]\n",
    "              'vect__tokenizer':[token.tokenize], #None, textblob_tokenizer,stemming_tokenizer, nltk.word_tokenize,my_tokenizer,token.tokenize\n",
    "              'vect__strip_accents': ['unicode'],# None,\n",
    "              'norm__norm': ['l2'], #,'l1'\n",
    "              'tfidf__norm': ['l1'], #, 'l2'\n",
    "              'tfidf__smooth_idf': [False],#True, \n",
    "              'tfidf__use_idf': [True],\n",
    "#               'clf__kernel':['rbf','linear','poly','sigmoid','precomputed'],\n",
    "#               'clf__gamma':[0.001, 0.01, 0.1, 1],\n",
    "#               'clf__degree':[1,2,3,4],\n",
    "#               'clf__nu':[0.2, 0.4, 0.5, 0.6, 0.8],\n",
    "              'clf__random_state': [42],\n",
    "              'clf__C':[10],\n",
    "              'clf__fit_intercept': [True], #, False\n",
    "             }\n",
    "\n",
    "grid_search_cv = model_selection.GridSearchCV(clf_pipeline_svm, parameters, cv=2, n_jobs=6, verbose=20)\n",
    "grid_search_cv.fit(X_train['text'], y_train)\n",
    "\n",
    "print('Best Parameters:', grid_search_cv.best_params_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = grid_search_cv.predict(X_validation['text'])\n",
    "\n",
    "print(metrics.classification.classification_report(y_validation, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 60 candidates, totalling 120 fits\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.8, clf__kernel=linear, clf__gamma=0.001, clf__degree=3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.8, clf__kernel=linear, clf__gamma=0.001, clf__degree=3, score=0.8804119588041196, total=10.0min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.8, clf__kernel=linear, clf__gamma=0.001, clf__degree=3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 13.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.8, clf__kernel=linear, clf__gamma=0.001, clf__degree=3, score=0.8865886588658866, total= 7.9min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.2, clf__kernel=linear, clf__gamma=1, clf__degree=3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 25.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.2, clf__kernel=linear, clf__gamma=1, clf__degree=3, score=0.9011098890110989, total=12.1min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.2, clf__kernel=linear, clf__gamma=1, clf__degree=3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 40.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.2, clf__kernel=linear, clf__gamma=1, clf__degree=3, score=0.9041904190419042, total=12.0min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.6, clf__kernel=linear, clf__gamma=0.01, clf__degree=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 55.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.6, clf__kernel=linear, clf__gamma=0.01, clf__degree=1, score=0.8982101789821018, total= 8.0min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.6, clf__kernel=linear, clf__gamma=0.01, clf__degree=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 66.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.6, clf__kernel=linear, clf__gamma=0.01, clf__degree=1, score=0.9007900790079008, total= 7.9min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.2, clf__kernel=poly, clf__gamma=0.001, clf__degree=3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 77.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.2, clf__kernel=poly, clf__gamma=0.001, clf__degree=3, score=0.8478152184781522, total= 1.7min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.2, clf__kernel=poly, clf__gamma=0.001, clf__degree=3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 79.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.2, clf__kernel=poly, clf__gamma=0.001, clf__degree=3, score=0.7038703870387039, total= 1.7min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.6, clf__kernel=sigmoid, clf__gamma=1, clf__degree=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 82.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.6, clf__kernel=sigmoid, clf__gamma=1, clf__degree=1, score=0.8988101189881011, total= 7.4min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.6, clf__kernel=sigmoid, clf__gamma=1, clf__degree=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 92.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.6, clf__kernel=sigmoid, clf__gamma=1, clf__degree=1, score=0.9006900690069007, total= 7.4min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.8, clf__kernel=linear, clf__gamma=1, clf__degree=1 \n",
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.8, clf__kernel=linear, clf__gamma=1, clf__degree=1, score=0.8804119588041196, total= 7.8min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.8, clf__kernel=linear, clf__gamma=1, clf__degree=1 \n",
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.8, clf__kernel=linear, clf__gamma=1, clf__degree=1, score=0.8865886588658866, total= 7.7min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.8, clf__kernel=rbf, clf__gamma=0.01, clf__degree=4 \n",
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.8, clf__kernel=rbf, clf__gamma=0.01, clf__degree=4, score=0.8807119288071192, total= 7.6min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.8, clf__kernel=rbf, clf__gamma=0.01, clf__degree=4 \n",
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.8, clf__kernel=rbf, clf__gamma=0.01, clf__degree=4, score=0.8864886488648865, total= 7.5min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.8, clf__kernel=sigmoid, clf__gamma=1, clf__degree=1 \n",
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.8, clf__kernel=sigmoid, clf__gamma=1, clf__degree=1, score=0.8806119388061194, total= 7.6min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.8, clf__kernel=sigmoid, clf__gamma=1, clf__degree=1 \n",
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.8, clf__kernel=sigmoid, clf__gamma=1, clf__degree=1, score=0.8865886588658866, total= 7.5min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.4, clf__kernel=sigmoid, clf__gamma=0.1, clf__degree=3 \n",
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.4, clf__kernel=sigmoid, clf__gamma=0.1, clf__degree=3, score=0.9015098490150985, total=10.7min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.4, clf__kernel=sigmoid, clf__gamma=0.1, clf__degree=3 \n",
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.4, clf__kernel=sigmoid, clf__gamma=0.1, clf__degree=3, score=0.9048904890489049, total=10.3min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.4, clf__kernel=rbf, clf__gamma=0.01, clf__degree=2 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.4, clf__kernel=rbf, clf__gamma=0.01, clf__degree=2, score=0.9013098690130987, total= 8.8min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.4, clf__kernel=rbf, clf__gamma=0.01, clf__degree=2 \n",
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.4, clf__kernel=rbf, clf__gamma=0.01, clf__degree=2, score=0.9048904890489049, total= 8.7min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.8, clf__kernel=linear, clf__gamma=0.001, clf__degree=1 \n",
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.8, clf__kernel=linear, clf__gamma=0.001, clf__degree=1, score=0.8804119588041196, total= 7.8min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.8, clf__kernel=linear, clf__gamma=0.001, clf__degree=1 \n",
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.8, clf__kernel=linear, clf__gamma=0.001, clf__degree=1, score=0.8865886588658866, total= 7.7min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.4, clf__kernel=poly, clf__gamma=0.1, clf__degree=4 \n",
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.4, clf__kernel=poly, clf__gamma=0.1, clf__degree=4, score=0.8599140085991401, total= 3.2min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.4, clf__kernel=poly, clf__gamma=0.1, clf__degree=4 \n",
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.4, clf__kernel=poly, clf__gamma=0.1, clf__degree=4, score=0.6811681168116812, total= 3.1min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.4, clf__kernel=linear, clf__gamma=0.1, clf__degree=1 \n",
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.4, clf__kernel=linear, clf__gamma=0.1, clf__degree=1, score=0.9015098490150985, total=12.1min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.4, clf__kernel=linear, clf__gamma=0.1, clf__degree=1 \n",
      "[CV]  vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.4, clf__kernel=linear, clf__gamma=0.1, clf__degree=1, score=0.9047904790479048, total=12.2min\n",
      "[CV] vect__tokenizer=<bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>, vect__strip_accents=unicode, vect__ngram_range=(1, 2), vect__max_features=None, vect__max_df=1.0, vect__binary=True, vect__analyzer=word, tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l1, norm__norm=l2, clf__random_state=42, clf__nu=0.5, clf__kernel=precomputed, clf__gamma=0.1, clf__degree=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:542: FutureWarning: From version 0.22, errors during fit will result in a cross validation score of NaN by default. Use error_score='raise' if you want an exception raised or error_score=np.nan to adopt the behavior from version 0.22.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Sparse precomputed kernels are not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-77fa5d37e016>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mrandom_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf_pipeline_svm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_distributions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mrandom_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1513\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[0;32m   1514\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1515\u001b[1;33m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0msparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msparse\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"precomputed\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sparse precomputed kernels are not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Sparse precomputed kernels are not supported."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as randint\n",
    "from scipy.stats import uniform\n",
    "\n",
    "seed = 42\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1,2)],#(1,1), (2,2),(1,3),(2,3)\n",
    "              'vect__max_features':[None],  # max (20000, 68354) #,10000,20000, 30000,40000, 50000,60000\n",
    "              'vect__binary': [True],#  , False\n",
    "              'vect__strip_accents': ['ascii'], #,'unicode',None\n",
    "              'vect__analyzer':['word'],# 'char', 'char_wb'\n",
    "              'vect__max_df' :[1.0], # 'C': [0.1, 1, 10, 100]\n",
    "              'vect__tokenizer':[token.tokenize], #None, textblob_tokenizer,stemming_tokenizer, nltk.word_tokenize,my_tokenizer,token.tokenize\n",
    "              'vect__strip_accents': ['unicode'],# None,\n",
    "              'norm__norm': ['l2'], #,'l1'\n",
    "              'tfidf__norm': ['l1'], #, 'l2'\n",
    "              'tfidf__smooth_idf': [False],#True, \n",
    "              'tfidf__use_idf': [True],\n",
    "#               'clf__kernel':['rbf','linear','poly','sigmoid'], #,'precomputed'\n",
    "#               'clf__gamma':[0.001, 0.01, 0.1, 1],\n",
    "#               'clf__degree':[1,2,3,4],\n",
    "#               'clf__nu':[0.2, 0.4, 0.5, 0.6, 0.8],\n",
    "#               'clf__random_state': [42],\n",
    "              'clf__C':[10],\n",
    "              'clf__fit_intercept': [True], #, False\n",
    "              \n",
    "             }\n",
    "\n",
    "random_search = RandomizedSearchCV(clf_pipeline_svm, param_distributions = parameters, cv=2, verbose = 10, random_state = seed, n_iter = 60)\n",
    "random_search.fit(X_train['text'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(random_search.cv_results_)\n",
    "y_pred = random_search.predict(X_validation['text'])\n",
    "print(metrics.classification_report(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on Test Set Using Our Best Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_predictions = grid_search_cv.predict(test_data['text'])\n",
    "data = {'Id' : test_data['id'], 'Category': test_set_predictions}\n",
    "submission = pd.DataFrame(data=data)\n",
    "submission = submission.apply(pd.to_numeric).sort_values(by=['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission6.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # submission 6 parameters\n",
    "\n",
    "# Best Parameters: {'clf__C': 10, 'clf__fit_intercept': True, 'clf__random_state': 42, 'norm__norm': 'l2', 'tfidf__norm': 'l1', 'tfidf__smooth_idf': False, 'tfidf__use_idf': True, 'vect__analyzer': 'word', 'vect__binary': True, 'vect__max_df': 1.0, 'vect__max_features': None, 'vect__ngram_range': (1, 2), 'vect__strip_accents': 'unicode', 'vect__tokenizer': <bound method RegexpTokenizer.tokenize of RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)>}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
