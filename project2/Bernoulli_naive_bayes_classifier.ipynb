{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import nltk\n",
    "import os  \n",
    "import re  \n",
    "import copy\n",
    "import math\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import stopwords \n",
    "from project_utilities import import_train_data, import_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\35904\\Desktop\\Mcgill_Study\\551ML\\project2\\train\\train')\n",
    "path = os.getcwd()\n",
    "\n",
    "train_data = import_train_data(path)\n",
    "#train_data = pd.DataFrame(import_train_data(path))\n",
    "os.chdir(r'C:\\Users\\35904\\Desktop\\Mcgill_Study\\551ML\\project2\\test\\test')\n",
    "path = os.getcwd()\n",
    "test_data = import_test_data(path)\n",
    "#test_data = pd.DataFrame(import_test_data(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find binary features\n",
    "\n",
    "def split_text (data_list):\n",
    "    \n",
    "    splited_text = []\n",
    "    result = [0]*len(data_list)\n",
    "    for index, value in enumerate(data_list):\n",
    "        result[index] = re.split(r'[\\s\\,\\;\\:\\.\\?\\!\\)\\(\\'\\\"\\/>\\<]+',value['text'].lower())\n",
    "    return result\n",
    "\n",
    "def concatenate_all_text(data_list):\n",
    "    \"\"\"\n",
    "    Concatenate and return each datapoint's text key value into one list\n",
    "\n",
    "    Arguments:\n",
    "    data_list -- list of dictionary type data to concatenate text from\n",
    "\n",
    "    Return:\n",
    "    List of all text from each data point from data_list\n",
    "    \"\"\"\n",
    "    all_text = []\n",
    "    splited_text = split_text(data_list)\n",
    "    for index, value in enumerate(splited_text):\n",
    "        all_text.extend(value)\n",
    "    return all_text\n",
    "\n",
    "def filter_stop_word (data_list):\n",
    "    \"\"\"\n",
    "    Function: it is used to filte the stop word out from the top words we have got\n",
    "\n",
    "    Arguments: \n",
    "    top_words -- List of top occuring words in the dataset\n",
    "\n",
    "    Return: \n",
    "    top words without stop word\n",
    "    \"\"\"\n",
    "    all_text = concatenate_all_text(data_list)\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    top_word_wsw = [w for w in all_text if not w in stop_words] #wsw means without stop word\n",
    "    return top_word_wsw\n",
    "\n",
    "\n",
    "def get_top_words(data_list, n_top_words):\n",
    "    \"\"\"\n",
    "    Get list of top words from given dataset\n",
    "\n",
    "    Arguments:\n",
    "    data_list -- Dataset to determine top words from \n",
    "    n_top_words -- Number of top words (default 160)\n",
    "\n",
    "    Return:\n",
    "    List of strings of the top 160 words\n",
    "    \"\"\"\n",
    "    n_top_words = n_top_words + 1\n",
    "    top_words = []\n",
    "    \n",
    "    d = Counter(filter_stop_word(data_list))\n",
    "    d_sorted = sorted(d.items(), key=itemgetter(1), reverse=True)\n",
    "    \n",
    "    assert len(d_sorted) >= n_top_words, 'Too many top words'\n",
    "    \n",
    "    for i in range(n_top_words):\n",
    "        top_words.append(d_sorted[i][0])\n",
    "        \n",
    "    while '' in top_words:\n",
    "        top_words.remove('')  # delate the empety element\n",
    "\n",
    "    return top_words\n",
    "\n",
    "def occurrence_top_words (data_point, top_words):\n",
    "    \n",
    "    occurrence = [0]*len(top_words)\n",
    "    \n",
    "    for index, value in enumerate (top_words):\n",
    "        occurrence[index] = int (value in data_point['text'])\n",
    "    \n",
    "    return occurrence\n",
    "\n",
    "def insert_top_words_occurrence (data_list,top_words):\n",
    "    result = copy.deepcopy(data_list)\n",
    "    \n",
    "    for index_result, value_result in enumerate (result):\n",
    "        top_words_occurrence = occurrence_top_words(value_result, top_words)\n",
    "        \n",
    "        for index_word, word in enumerate (top_words_occurrence):\n",
    "            column_name = 'top_word_occurrence_' + str(index_word + 1).zfill(3)\n",
    "            result[index_result][column_name] = top_words_occurrence[index_word]\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_num_pos_neg_train_data (dictionary):\n",
    "    os.chdir(dictionary)\n",
    "    path = os.getcwd()\n",
    "\n",
    "    path_pos = os.path.join(path,'pos')\n",
    "    path_neg = os.path.join(path,'neg')\n",
    "\n",
    "    num_pos = len(os.listdir(path_pos))\n",
    "    num_neg = len(os.listdir(path_neg))\n",
    "    \n",
    "    return num_pos,num_neg\n",
    "\n",
    "def get_probability_pos_neg (num_pos,num_neg):\n",
    "    \n",
    "    Pro_pos = num_pos/(num_pos+num_neg)\n",
    "    Pro_neg = num_neg/(num_pos+num_neg)\n",
    "    \n",
    "    return Pro_pos, Pro_neg\n",
    "\n",
    "def split_dataset_to_pos_neg (data_list, num_pos, num_neg):\n",
    "    data_list_pos = [0]*num_pos\n",
    "    data_list_neg = [0]*num_neg\n",
    "    index_pos = int (0)\n",
    "    index_neg = int (0)\n",
    "    for index, value in enumerate (data_list):\n",
    "        if data_list[index]['category'] == 1:\n",
    "            data_list_pos[index_pos] = data_list[index]\n",
    "            index_pos = index_pos + 1\n",
    "        else:\n",
    "            data_list_neg[index_neg] = data_list[index]\n",
    "            index_neg = index_neg + 1\n",
    "    return data_list_pos, data_list_neg\n",
    "\n",
    "def get_pro_x1_given_y1_or_x1_given_y0 (input_data):\n",
    "    \n",
    "    data_list = copy.deepcopy(input_data)\n",
    "    \n",
    "    for index, value in enumerate (data_list):\n",
    "        del data_list[index]['category']\n",
    "        del data_list[index]['text']\n",
    "    \n",
    "    features_list = list(data_list[0].keys())\n",
    "    length_pos = len(data_list)\n",
    "    Pro_pos_feature = [0]*len(features_list)\n",
    "    add_up = 0\n",
    "\n",
    "    for index_feature, value_feature in enumerate (features_list):\n",
    "        for index_data, value_data in enumerate (data_list):\n",
    "            if data_list[index_data][value_feature] == 1:\n",
    "                add_up = add_up + 1\n",
    "        Pro_pos_feature[index_feature] = (add_up+1)/(length_pos+2)\n",
    "        add_up = 0\n",
    "    \n",
    "    return Pro_pos_feature\n",
    "\n",
    "def pre_processing_test_data (input_data):\n",
    "    \n",
    "    id_num = []\n",
    "    data_list = copy.deepcopy(input_data)\n",
    "    \n",
    "    for index, value in enumerate (data_list):\n",
    "        id_num.append(value['id'])\n",
    "        del value['id']\n",
    "        del value['text']\n",
    "\n",
    "    return data_list, id_num\n",
    "\n",
    "def make_decision (input_data):\n",
    "    \n",
    "    decision_value = 0\n",
    "    decision = [0]*len(input_data)\n",
    "    data_list = copy.deepcopy(input_data)\n",
    "    features_list = list(data_list[0].keys())\n",
    "    \n",
    "    for index, value in enumerate (data_list):\n",
    "        for index_feature, value_feature in enumerate (features_list):\n",
    "            decision_value = decision_value + value[value_feature]* math.log10(pro_x1_given_y1[index_feature]/pro_x1_given_y0[index_feature])+ (1-value[value_feature])*math.log10((1-pro_x1_given_y1[index_feature])/(1-pro_x1_given_y0[index_feature]))\n",
    "        if decision_value > 0:\n",
    "            decision[index] = 1\n",
    "        else:\n",
    "            decision[index] = 0\n",
    "        decision_value = 0\n",
    "    return decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = get_top_words(train_data, 150)\n",
    "train_data = insert_top_words_occurrence (train_data, top_words)\n",
    "test_data = insert_top_words_occurrence(test_data,top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pos,num_neg = get_num_pos_neg_train_data(r'C:\\Users\\35904\\Desktop\\Mcgill_Study\\551ML\\project2\\train\\train')\n",
    "Pro_pos, Pro_neg = get_probability_pos_neg(num_pos,num_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_pos,train_data_neg =  split_dataset_to_pos_neg (train_data, num_pos, num_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_x1_given_y1 = get_pro_x1_given_y1_or_x1_given_y0(train_data_pos)\n",
    "pro_x1_given_y0 = get_pro_x1_given_y1_or_x1_given_y0(train_data_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data1,id_num = pre_processing_test_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = []\n",
    "for index, value in enumerate (train_data):\n",
    "    category.append(value['category'])\n",
    "    del train_data[index]['category']\n",
    "    del train_data[index]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = make_decision(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "currect_sum = 0\n",
    "\n",
    "for index, value in enumerate (category):\n",
    "    if decision[index] == value:\n",
    "        currect_sum = currect_sum + 1\n",
    "        \n",
    "currect_rate = currect_sum/len(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74572"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currect_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
