{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re  \n",
    "import math\n",
    "import sklearn\n",
    "from project_utilities import import_train_data, import_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get train_data and test_data from files："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\35904\\Desktop\\Mcgill_Study\\551ML\\project2\\train\\train')\n",
    "path = os.getcwd()\n",
    "row_data = import_train_data(path)\n",
    "\n",
    "os.chdir(r'C:\\Users\\35904\\Desktop\\Mcgill_Study\\551ML\\project2\\test\\test')\n",
    "path = os.getcwd()\n",
    "test_data = import_test_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define some functions："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import stopwords \n",
    "import copy\n",
    "import nltk\n",
    "\n",
    "def split_train_validation_data (input_data):\n",
    "    data_list = copy.deepcopy(input_data)\n",
    "    train_data_pos = data_list[0:10000]\n",
    "    validation_data_pos = data_list[10000:12500]\n",
    "    train_data_neg = data_list[12500:22500]\n",
    "    validation_data_neg = data_list[22500:25000]\n",
    "    train_data_pos.extend(train_data_neg)\n",
    "    validation_data_pos.extend(validation_data_neg)\n",
    "    \n",
    "    return train_data_pos, validation_data_pos\n",
    "\n",
    "def split_text (data_list):\n",
    "    \n",
    "    splited_text = []\n",
    "    result = [0]*len(data_list)\n",
    "    for index, value in enumerate(data_list):\n",
    "        result[index] = re.split(r'[\\s\\,\\;\\:\\.\\?\\!\\)\\(\\'\\\"\\/>\\<]+',value['text'].lower())\n",
    "    return result\n",
    "\n",
    "def concatenate_all_text(data_list):\n",
    "    \"\"\"\n",
    "    Concatenate and return each datapoint's text key value into one list\n",
    "\n",
    "    Arguments:\n",
    "    data_list -- list of dictionary type data to concatenate text from\n",
    "\n",
    "    Return:\n",
    "    List of all text from each data point from data_list\n",
    "    \"\"\"\n",
    "    all_text = []\n",
    "    splited_text = split_text(data_list)\n",
    "    for index, value in enumerate(splited_text):\n",
    "        all_text.extend(value)\n",
    "    return all_text\n",
    "\n",
    "def filter_stop_word (data_list):\n",
    "    \"\"\"\n",
    "    Function: it is used to filte the stop word out from the top words we have got\n",
    "\n",
    "    Arguments: \n",
    "    top_words -- List of top occuring words in the dataset\n",
    "\n",
    "    Return: \n",
    "    top words without stop word\n",
    "    \"\"\"\n",
    "    all_text = concatenate_all_text(data_list)\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    top_word_wsw = [w for w in all_text if not w in stop_words] #wsw means without stop word\n",
    "    return top_word_wsw\n",
    "\n",
    "\n",
    "def get_top_words(data_list, n_top_words):\n",
    "    \"\"\"\n",
    "    Get list of top words from given dataset\n",
    "\n",
    "    Arguments:\n",
    "    data_list -- Dataset to determine top words from \n",
    "    n_top_words -- Number of top words\n",
    "\n",
    "    Return:\n",
    "    List of strings of the top words\n",
    "    \"\"\"\n",
    "    n_top_words = n_top_words + 1 # we will delete empety element latter,so add 1 here\n",
    "    top_words = []\n",
    "    \n",
    "    d = Counter(filter_stop_word(data_list))\n",
    "    d_sorted = sorted(d.items(), key=itemgetter(1), reverse=True)\n",
    "    \n",
    "    assert len(d_sorted) >= n_top_words, 'Too many top words'\n",
    "    \n",
    "    for i in range(n_top_words):\n",
    "        top_words.append(d_sorted[i][0])\n",
    "        \n",
    "    while '' in top_words:\n",
    "        top_words.remove('')  # delate the empety element\n",
    "\n",
    "    return top_words\n",
    "\n",
    "def occurrence_top_words (data_point, top_words):\n",
    "    '''\n",
    "    Function is '1' for occurrence of a word, '0' for not occurrence of a word\n",
    "\n",
    "    Argument:\n",
    "    data_point:\n",
    "    top_words:\n",
    "\n",
    "    Return:\n",
    "    A list of top words occurrence\n",
    "\n",
    "    '''\n",
    "    occurrence = [0]*len(top_words)\n",
    "    \n",
    "    for index, value in enumerate (top_words):\n",
    "        occurrence[index] = int (value in data_point['text'])\n",
    "    \n",
    "    return occurrence\n",
    "\n",
    "def insert_top_words_occurrence (data_list,top_words):\n",
    "    result = copy.deepcopy(data_list)\n",
    "    \n",
    "    for index_result, value_result in enumerate (result):\n",
    "        top_words_occurrence = occurrence_top_words(value_result, top_words)\n",
    "        \n",
    "        for index_word, word in enumerate (top_words_occurrence):\n",
    "            column_name = 'top_word_occurrence_' + str(index_word + 1).zfill(3)\n",
    "            result[index_result][column_name] = top_words_occurrence[index_word]\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_num_pos_neg_train_data (input_data):\n",
    "    \n",
    "    data_list = copy.deepcopy(input_data)\n",
    "    \n",
    "    counter_pos = 0\n",
    "    counter_neg = 0\n",
    "    \n",
    "    for index, value in enumerate (data_list):\n",
    "        if data_list[index]['category'] == 1:\n",
    "            counter_pos = counter_pos + 1\n",
    "        else:\n",
    "            counter_neg = counter_neg + 1\n",
    "    return counter_pos,counter_neg\n",
    "\n",
    "\n",
    "def get_probability_pos_neg (num_pos,num_neg):\n",
    "    \n",
    "    Pro_pos = num_pos/(num_pos+num_neg)\n",
    "    Pro_neg = num_neg/(num_pos+num_neg)\n",
    "    \n",
    "    return Pro_pos, Pro_neg\n",
    "\n",
    "def split_dataset_to_pos_neg (input_data, num_pos, num_neg):\n",
    "    \n",
    "    data_list = copy.deepcopy(input_data)\n",
    "    data_list_pos = [0]*num_pos\n",
    "    data_list_neg = [0]*num_neg\n",
    "    index_pos = int (0)\n",
    "    index_neg = int (0)\n",
    "    for index, value in enumerate (data_list):\n",
    "        if data_list[index]['category'] == 1:\n",
    "            data_list_pos[index_pos] = data_list[index]\n",
    "            index_pos = index_pos + 1\n",
    "        else:\n",
    "            data_list_neg[index_neg] = data_list[index]\n",
    "            index_neg = index_neg + 1\n",
    "    return data_list_pos, data_list_neg\n",
    "\n",
    "def get_pro_x1_given_y1_or_x1_given_y0 (input_data):\n",
    "    \n",
    "    data_list = copy.deepcopy(input_data)\n",
    "    \n",
    "    for index, value in enumerate (data_list):\n",
    "        del data_list[index]['category']\n",
    "        del data_list[index]['text']\n",
    "    \n",
    "    features_list = list(data_list[0].keys())\n",
    "    length_pos = len(data_list)\n",
    "    Pro_pos_feature = [0]*len(features_list)\n",
    "    add_up = 0\n",
    "\n",
    "    for index_feature, value_feature in enumerate (features_list):\n",
    "        for index_data, value_data in enumerate (data_list):\n",
    "            if data_list[index_data][value_feature] == 1:\n",
    "                add_up = add_up + 1\n",
    "        Pro_pos_feature[index_feature] = (add_up+1)/(length_pos+2)\n",
    "        add_up = 0\n",
    "    \n",
    "    return Pro_pos_feature\n",
    "\n",
    "def make_decision (input_data):\n",
    "    \n",
    "    decision_value = 0\n",
    "    decision = [0]*len(input_data)\n",
    "    data_list = copy.deepcopy(input_data)\n",
    "    \n",
    "    features_list = list(data_list[0].keys())\n",
    "    \n",
    "    for index, value in enumerate (data_list):\n",
    "        for index_feature, value_feature in enumerate (features_list):\n",
    "            decision_value = decision_value + value[value_feature]* math.log10(pro_x1_given_y1[index_feature]/pro_x1_given_y0[index_feature])+ (1-value[value_feature])*math.log10((1-pro_x1_given_y1[index_feature])/(1-pro_x1_given_y0[index_feature]))\n",
    "            # because p(y = 1) = p(y = 0),the log(p(y = 1)/p(y = 0)) = 0, so ignore it\n",
    "        if decision_value > 0:\n",
    "            decision[index] = 1\n",
    "        else:\n",
    "            decision[index] = 0\n",
    "        decision_value = 0\n",
    "    return decision\n",
    "\n",
    "def get_correction_rate (classification, reference):\n",
    "\n",
    "    currect_sum = 0\n",
    "    for index, value in enumerate (reference):\n",
    "        if classification[index] == value:\n",
    "            currect_sum = currect_sum + 1\n",
    "    currect_rate = currect_sum/len(reference)\n",
    "    \n",
    "    return currect_rate\n",
    "\n",
    "def get_category_list (input_data):\n",
    "    data_list = copy.deepcopy(input_data)\n",
    "    category = []\n",
    "    for index, value in enumerate (data_list):\n",
    "        category.append(value['category'])\n",
    "    return category\n",
    "\n",
    "def pre_processing_test_data (input_data):\n",
    "    \n",
    "    id_num = []\n",
    "    data_list = copy.deepcopy(input_data)\n",
    "    \n",
    "    for index, value in enumerate (data_list):\n",
    "        id_num.append(value['id'])\n",
    "        del value['id']\n",
    "        del value['text']\n",
    "\n",
    "    return data_list, id_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train_data (20000) and validation data(5000)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data = split_train_validation_data(row_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top_words features and insert them into data set："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-a496fc419d73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_top_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minsert_top_words_occurrence\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minsert_top_words_occurrence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-5b5d0333c9fb>\u001b[0m in \u001b[0;36minsert_top_words_occurrence\u001b[1;34m(data_list, top_words)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtop_words_occurrence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mcolumn_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'top_word_occurrence_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_word\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex_result\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtop_words_occurrence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "top_words = get_top_words(train_data, 10000)\n",
    "train_data = insert_top_words_occurrence (train_data, top_words)\n",
    "validation_data = insert_top_words_occurrence(validation_data,top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get number of positive comment and negative comment and then calculate their probability："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pos,num_neg = get_num_pos_neg_train_data(train_data)\n",
    "Pro_pos, Pro_neg = get_probability_pos_neg(num_pos,num_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the train_data into postive data set and negative data set："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_pos,train_data_neg =  split_dataset_to_pos_neg (train_data, num_pos, num_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the probabiliy of xj = 1 by giving y = 1 or y = 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_x1_given_y1 = get_pro_x1_given_y1_or_x1_given_y0(train_data_pos)\n",
    "pro_x1_given_y0 = get_pro_x1_given_y1_or_x1_given_y0(train_data_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a list of category (used to comparation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_train_data = get_category_list(train_data)\n",
    "category_validation_data = get_category_list(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make decision and calculate correction rate on train_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, value in enumerate (train_data):\n",
    "    del train_data[index]['category']\n",
    "    del train_data[index]['text']\n",
    "\n",
    "decision_train_data = make_decision(train_data)\n",
    "\n",
    "for index, value in enumerate (validation_data):\n",
    "    del validation_data[index]['category']\n",
    "    del validation_data[index]['text']\n",
    "\n",
    "decision_validation_data = make_decision(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currect_rate_train_data = get_correction_rate(decision_train_data, category_train_data)\n",
    "currect_rate_validation_data = get_correction_rate(decision_validation_data, category_validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('currention rate of train_data is :', currect_rate_train_data)\n",
    "print('currention rate of validation_data is :', currect_rate_validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make prediction on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = insert_top_words_occurrence (test_data, top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data,id_num = pre_processing_test_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_test_data = make_decision(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
