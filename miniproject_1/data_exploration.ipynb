{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "\n",
    "file_name_data = 'proj1_data.json'\n",
    "\n",
    "with open(file_name_data) as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_list):\n",
    "    \"\"\"\n",
    "    brief: Converts text data to lower case, splits it by whitespace, and encodes the is_root feature on json data point provided by projmaterials1\n",
    "    param data: list of dictionary type data to perform text lower case conversion, text splitting, and is_root encoding\n",
    "    return: list of dictionary type data that's been processed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Deep copy data as to avoid overwriting which could cause unintented side effects\n",
    "    result = copy.deepcopy(data_list)\n",
    "    for index, value in enumerate(result):\n",
    "        result[index]['text'] = value['text'].lower().split(' ')\n",
    "        result[index]['is_root'] = int(value['is_root'] == 'true')\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_all_text(data_list):\n",
    "    \n",
    "    all_text = []\n",
    "    for index, value in enumerate(data_list):\n",
    "        all_text.extend(value['text'])\n",
    "    \n",
    "    return all_text\n",
    "\n",
    "def get_top_words(data_list, n_top_words=160):\n",
    "    \n",
    "    top_words = []\n",
    "    \n",
    "    d = Counter(concatenate_all_text(data_list))\n",
    "    d_sorted = sorted(d.items(), key=itemgetter(1), reverse=True)\n",
    "    \n",
    "    assert len(d_sorted) >= n_top_words, 'Too many top words'\n",
    "    \n",
    "    for i in range(n_top_words):\n",
    "        top_words.append(d_sorted[i][0])\n",
    "        \n",
    "    return top_words\n",
    "\n",
    "def get_top_words_count(data_point, top_words):\n",
    "    \n",
    "    word_count = np.zeros(len(top_words))\n",
    "    \n",
    "    for index, word in enumerate(top_words):\n",
    "        word_count[index] = data_point['text'].count(word)\n",
    "    \n",
    "    return word_count\n",
    "\n",
    "def insert_top_words_count_to_data(data_list, top_words):\n",
    "    \n",
    "    result = copy.deepcopy(data_list)\n",
    "    \n",
    "    for index_example, example in enumerate(result):\n",
    "        top_words_count = get_top_words_count(example, top_words)\n",
    "        \n",
    "        for index_word, word in enumerate(top_words_count):\n",
    "            column_name = 'top_word_' + str(index_word + 1)\n",
    "            result[index_example][column_name] = np.int32(top_words_count[index_word])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Training, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = process_data(data)\n",
    "X_train = X[0:10000]\n",
    "X_validation = X[10000:11000]\n",
    "X_test = X[11000:]\n",
    "\n",
    "assert len(X_train) == 10000 , 'Expected 10000. Got %d' % len(X_train)\n",
    "assert len(X_validation) == 1000 , 'Expected 1000. Got %d' % len(X_validation)\n",
    "assert len(X_test) == 1000 , 'Expected 1000. Got %d' % len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_160_words = get_top_words(X_train)\n",
    "assert len(top_160_words) == 160, 'Expected 160. Got %d' % len(top_160_words)\n",
    "\n",
    "X_train = insert_top_words_count_to_data(X_train, top_160_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_by_key(matrix, key):\n",
    "    try:\n",
    "        return [row[key] for row in matrix]\n",
    "    except:\n",
    "        print('No such key in matrix')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_column_by_key(matrix, key):\n",
    "    result = copy.deepcopy(matrix)\n",
    "    \n",
    "    try:\n",
    "        for index in range(len(result)):\n",
    "            del result[index][key]\n",
    "    except:\n",
    "        print('No such key in matrix')\n",
    "        pass\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_gradient_descent(beta = 1, eta = 20)\n",
    "beta = 1\n",
    "eta = 1\n",
    "alpha = eta / (1 + beta)\n",
    "weights_old = np.ones(164)\n",
    "\n",
    "y = get_column_by_key(X_train, 'popularity_score')\n",
    "X_train = delete_column_by_key(X_train, 'popularity_score')\n",
    "X_train = delete_column_by_key(X_train, 'text')\n",
    "\n",
    "# Include bias\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i]['bias'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreate X Training Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_train)\n",
    "X_train_values = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = weights_old - 2 * alpha * \\\n",
    "    (np.dot(\n",
    "        np.dot(\n",
    "            X_train_values.T, X_train_values), \n",
    "        weights_old) - np.dot(X_train_values.T, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
