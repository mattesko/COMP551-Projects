{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "\n",
    "file_name_data = 'proj1_data.json'\n",
    "\n",
    "with open(file_name_data) as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_list):\n",
    "    \"\"\"\n",
    "    brief: Converts text data to lower case, splits it by whitespace, and encodes the is_root feature on json data point provided by projmaterials1\n",
    "    param data: list of dictionary type data to perform text lower case conversion, text splitting, and is_root encoding\n",
    "    return: list of dictionary type data that's been processed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Deep copy data as to avoid overwriting which could cause unintented side effects\n",
    "    result = copy.deepcopy(data_list)\n",
    "    for index, value in enumerate(result):\n",
    "        result[index]['text'] = value['text'].lower().split(' ')\n",
    "        result[index]['is_root'] = int(value['is_root'] == 'true')\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_all_text(data_list):\n",
    "    \n",
    "    all_text = []\n",
    "    for index, value in enumerate(data_list):\n",
    "        all_text.extend(value['text'])\n",
    "    \n",
    "    return all_text\n",
    "\n",
    "def get_top_words(data_list, n_top_words=160):\n",
    "    \n",
    "    top_words = []\n",
    "    \n",
    "    d = Counter(concatenate_all_text(data_list))\n",
    "    d_sorted = sorted(d.items(), key=itemgetter(1), reverse=True)\n",
    "    \n",
    "    assert len(d_sorted) >= n_top_words, 'Too many top words'\n",
    "    \n",
    "    for i in range(n_top_words):\n",
    "        top_words.append(d_sorted[i][0])\n",
    "        \n",
    "    return top_words\n",
    "\n",
    "def get_top_words_count(data_point, top_words):\n",
    "    \n",
    "    word_count = np.zeros(len(top_words))\n",
    "    \n",
    "    for index, word in enumerate(top_words):\n",
    "        word_count[index] = data_point['text'].count(word)\n",
    "    \n",
    "    return word_count\n",
    "\n",
    "def add_top_words_count_to_data(data_list, top_words):\n",
    "    \n",
    "    result = copy.deepcopy(data_list)\n",
    "    for index, value in enumerate(result):\n",
    "        top_word_count = get_top_words_count(value, top_words)\n",
    "        result[index]['top_word_count'] = top_word_count\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Training, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = process_data(data)\n",
    "X_train = X[0:10000]\n",
    "X_validation = X[10000:11000]\n",
    "X_test = X[11000:]\n",
    "\n",
    "assert len(X_train) == 10000 , 'Expected 10000. Got %d' % len(X_train)\n",
    "assert len(X_validation) == 1000 , 'Expected 1000. Got %d' % len(X_validation)\n",
    "assert len(X_test) == 1000 , 'Expected 1000. Got %d' % len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_160_words = get_top_words(X_train)\n",
    "assert len(top_160_words) == 160, 'Expected 160. Got %d' % len(top_160_words)\n",
    "\n",
    "X_train = add_top_words_count_to_data(X_train, top_160_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
