{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text : ITS RAINING SIDEWAYS\n",
      "is_root : False\n",
      "controversiality : 0\n",
      "children : 0\n",
      "popularity_score : 1.254698160267241\n"
     ]
    }
   ],
   "source": [
    "import json # we need to use the JSON package to load the data, since the data is stored in JSON format\n",
    "\n",
    "with open(\"proj1_data.json\") as fp:\n",
    "    data = json.load(fp)\n",
    "    \n",
    "# Now the data is loaded.\n",
    "# It a list of data points, where each datapoint is a dictionary with the following attributes:\n",
    "# popularity_score : a popularity score for this comment (based on the number of upvotes) (type: float)\n",
    "# children : the number of replies to this comment (type: int)\n",
    "# text : the text of this comment (type: string)\n",
    "# controversiality : a score for how \"controversial\" this comment is (automatically computed by Reddit)\n",
    "# is_root : if True, then this comment is a direct reply to a post; \n",
    "# if False, this is a direct reply to another comment \n",
    "\n",
    "# Example:\n",
    "data_point = data[0] # select the first data point in the dataset\n",
    "\n",
    "# Now we print all the information about this datapoint\n",
    "for info_name, info_value in data_point.items():\n",
    "    print(info_name + \" : \" + str(info_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data_point['is_root'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "\n",
    "file_name_data = 'proj1_data.json'\n",
    "\n",
    "with open(file_name_data) as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_list):\n",
    "    \"\"\"\n",
    "    brief: Converts text data to lower case, splits it by whitespace, and encodes the is_root feature on \n",
    "    json data point provided by projmaterials1 param data: list of dictionary type data to perform text \n",
    "    lower case conversion, text splitting, and is_root encoding return: list of dictionary type data that's \n",
    "    been processed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Deep copy data as to avoid overwriting which could cause unintented side effects\n",
    "    result = copy.deepcopy(data_list)\n",
    "    for index, value in enumerate(result):\n",
    "        result[index]['text'] = value['text'].lower().split(' ')\n",
    "        result[index]['is_root'] = int(value['is_root'] == True)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_all_text(data_list):\n",
    "    \n",
    "    all_text = []\n",
    "    for index, value in enumerate(data_list):\n",
    "        all_text.extend(value['text'])\n",
    "    \n",
    "    return all_text\n",
    "\n",
    "def get_top_words(data_list, n_top_words=160):\n",
    "    \n",
    "    top_words = []\n",
    "    \n",
    "    d = Counter(concatenate_all_text(data_list))\n",
    "    d_sorted = sorted(d.items(), key=itemgetter(1), reverse=True)\n",
    "    \n",
    "    assert len(d_sorted) >= n_top_words, 'Too many top words'\n",
    "    \n",
    "    for i in range(n_top_words):\n",
    "        top_words.append(d_sorted[i][0])\n",
    "        \n",
    "    return top_words    \n",
    "    \n",
    "def get_top_words_count(data_point, top_words):\n",
    "    \n",
    "    word_count = np.zeros(len(top_words)) \n",
    "    \n",
    "    for index, word in enumerate(top_words):\n",
    "        word_count[index] = data_point['text'].count(word)\n",
    "    \n",
    "    return word_count\n",
    "\n",
    "def add_top_words_count_to_data(data_list, top_words):\n",
    "    \n",
    "    result = copy.deepcopy(data_list)\n",
    "    for index, value in enumerate(result):\n",
    "        top_word_count = get_top_words_count(value, top_words)\n",
    "        result[index]['top_word_count'] = top_word_count\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = process_data (data)\n",
    "top_words = get_top_words(preprocessed_data)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Training, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = process_data(data)\n",
    "X_train = X[0:10000]\n",
    "X_validation = X[10000:11000]\n",
    "X_test = X[11000:]\n",
    "\n",
    "#print(len(X_validation))\n",
    "\n",
    "assert len(X_train) == 10000 , 'Expected 10000. Got %d' % len(X_train)\n",
    "assert len(X_validation) == 1000 , 'Expected 1000. Got %d' % len(X_validation)\n",
    "assert len(X_test) == 1000 , 'Expected 1000. Got %d' % len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_160_words = get_top_words(X_train)\n",
    "assert len(top_160_words) == 160, 'Expected 160. Got %d' % len(top_160_words)\n",
    "\n",
    "X_train = add_top_words_count_to_data(X_train, top_160_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.zeros(shape = (10000,1));\n",
    "for index in range(0, len(X_train)):\n",
    "    Y_train[index] = X_train[index]['popularity_score']\n",
    "    del(X_train[index]['popularity_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rearrange input array, 10000 examples by 164 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_values = np.zeros(shape = (10000 , 164))\n",
    "for index in range(0, len(X_train)):\n",
    "    X_train_values[index][0] = X_train[index][\"is_root\"]\n",
    "    X_train_values[index][1] = X_train[index]['controversiality']\n",
    "    X_train_values[index][2] = X_train[index]['children']\n",
    "    top_word_count = X_train[index]['top_word_count']\n",
    "    for array_index in range(0, len(top_word_count)):\n",
    "        X_train_values[index][array_index + 3] = top_word_count[array_index]\n",
    "    X_train_values[index][163] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed-form approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trasnposed = X_train_values.transpose()\n",
    "X_trasnposed_X = X_trasnposed.dot(X_train_values)\n",
    "X_trasnposed_X_inverse = np.linalg.inv(X_trasnposed_X)\n",
    "\n",
    "# Y_train is a list, transform it to an array and size is 10000*1\n",
    "#Y_train = np.array(Y_train)\n",
    "#Y_train = Y_train.reshape((10000, 1))\n",
    "#print(Y_train.shape)\n",
    "\n",
    "X_trasnposed_Y = X_trasnposed.dot(Y_train)\n",
    "weight_coefficient = X_trasnposed_X_inverse.dot(X_trasnposed_Y)\n",
    "\n",
    "weight_coefficient.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Validation set (closed-form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_validation = np.zeros(shape = (1000 , 1))\n",
    "for index in range(0, len(X_validation)):\n",
    "    Y_validation[index] = X_validation[index]['popularity_score']\n",
    "    del(X_validation[index]['popularity_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation = add_top_words_count_to_data(X_validation, top_160_words)\n",
    "\n",
    "X_validation_values = np.zeros(shape = (1000 , 164))\n",
    "for index in range(0, len(X_validation)):\n",
    "    X_validation_values[index][0] = X_validation[index][\"is_root\"]\n",
    "    X_validation_values[index][1] = X_validation[index]['controversiality']\n",
    "    X_validation_values[index][2] = X_validation[index]['children']\n",
    "    top_word_count = X_validation[index]['top_word_count']\n",
    "    for array_index in range(0, len(top_word_count)):\n",
    "        X_validation_values[index][array_index + 3] = top_word_count[array_index]\n",
    "    X_validation_values[index][163] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_valid_pridiction = X_validation_values.dot(weight_coefficient)\n",
    "Y_valid_pridiction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display results (Validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.84333697  0.61270368]\n",
      " [ 0.89400237  0.79462912]\n",
      " [ 3.42605184  1.52805469]\n",
      " ..., \n",
      " [ 0.65148906  0.26863527]\n",
      " [ 1.01984666  0.78444281]\n",
      " [-0.74624472  2.06937147]]\n"
     ]
    }
   ],
   "source": [
    "comparation_vlidation = np.hstack ([Y_validation,Y_valid_pridiction])\n",
    "print(comparation_vlidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.9895357]\n"
     ]
    }
   ],
   "source": [
    "# calculate the MES\n",
    "MSE_validation = ((Y_validation - Y_valid_pridiction)**2).mean(axis=0)\n",
    "print(MSE_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display results (Training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pridiction = X_train_values.dot(weight_coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.82884594  1.25469816]\n",
      " [ 0.75712377  0.50981271]\n",
      " [ 0.60203502  0.3708268 ]\n",
      " ..., \n",
      " [ 0.42636679  0.15810991]\n",
      " [ 0.84591094  0.89307136]\n",
      " [ 0.59971335  0.14033016]]\n"
     ]
    }
   ],
   "source": [
    "Y_train = np.array(Y_train)\n",
    "comparation_train = np.hstack ([Y_train_pridiction,Y_train])\n",
    "print(comparation_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.04683291]\n"
     ]
    }
   ],
   "source": [
    "MSE_train = ((Y_train - Y_train_pridiction)**2).mean(axis=0)\n",
    "print(MSE_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
