{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mystery\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from collections import defaultdict\n",
    "import pytreebank\n",
    "import gensim\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Conv1D, MaxPooling1D, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import Constant\n",
    "import tensorflow.keras.backend\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use nltk to remove stopwords and lemmatize\n",
    "# you might need to run: nltk.download() to fetch the stopword package in \"all packages\"\n",
    "# you might also need to run ntlk.download(\"punkt\")\n",
    "english_stopwords = set(stopwords.words(\"english\"))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_input_text(text):\n",
    "    clean_text = []\n",
    "    for sent in text:\n",
    "        clean_sent = \"\"\n",
    "        sent_tokens = word_tokenize(sent)\n",
    "        for token in sent_tokens:\n",
    "            clean_sent += wordnet_lemmatizer.lemmatize(token) + \" \" if token not in english_stopwords else \"\"\n",
    "        clean_text.append(clean_sent)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for the cnn\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.2\n",
    "NUMBER_DIFFERENT_OUTPUTS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load pretrain glove word2vec instance for preprocessing\n",
    "filename = './data/glove.6B.300d.txt'\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(filename, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1101"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load SST1\n",
    "dataset = pytreebank.load_sst(\"./data/stanfordSentimentTreebank\")\n",
    "\n",
    "example = dataset[\"dev\"]\n",
    "len(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][1].to_labeled_lines()\n",
    "\n",
    "# retrieve all sentences and their labels from the sst dataset\n",
    "def retrieve_examples_and_labels(dataset_name):\n",
    "    labels, x_text = [],[]\n",
    "    for lines in dataset[dataset_name]:\n",
    "        labeled_lines = lines.to_labeled_lines()\n",
    "        for label, example in labeled_lines:\n",
    "            labels.append(label)\n",
    "            x_text.append(example)\n",
    "    return x_text, labels\n",
    "\n",
    "x_text1, y_train1 = retrieve_examples_and_labels(\"train\")\n",
    "x_text2, y_train2 = retrieve_examples_and_labels(\"test\")\n",
    "x_text = clean_input_text(x_text1 + x_text2)\n",
    "y = y_train1 + y_train2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15844\n",
      "Shape of data tensor: (401182, 1000)\n",
      "Shape of label tensor: (401182, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize the input text (both negative and positive )\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(x_text)\n",
    "sequences = tokenizer.texts_to_sequences(x_text)\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(y))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "labels[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80236"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(NUMBER_DIFFERENT_OUTPUTS, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 1000, 300)         4753500   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 996, 128)          192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 5,126,881\n",
      "Trainable params: 373,381\n",
      "Non-trainable params: 4,753,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create a datagenerator inheriting from utils.Sequence\n",
    "# in order to feed data by batches to the cnn to avoid\n",
    "# gpu problems.\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 320946 samples, validate on 80236 samples\n",
      "Epoch 1/10\n",
      "320946/320946 [==============================] - 173s 538us/step - loss: 0.7874 - acc: 0.7306 - val_loss: 0.7630 - val_acc: 0.7411\n",
      "Epoch 2/10\n",
      "320946/320946 [==============================] - 168s 525us/step - loss: 0.7387 - acc: 0.7489 - val_loss: 0.7482 - val_acc: 0.7449\n",
      "Epoch 3/10\n",
      "320946/320946 [==============================] - 178s 553us/step - loss: 0.7157 - acc: 0.7590 - val_loss: 0.7376 - val_acc: 0.75034s - los - ETA: 2s - loss: 0.7156 - ac\n",
      "Epoch 4/10\n",
      "320946/320946 [==============================] - 173s 538us/step - loss: 0.7007 - acc: 0.7662 - val_loss: 0.7593 - val_acc: 0.7499\n",
      "Epoch 5/10\n",
      "320946/320946 [==============================] - 168s 524us/step - loss: 0.6918 - acc: 0.7707 - val_loss: 0.7754 - val_acc: 0.7511 -\n",
      "Epoch 6/10\n",
      "320946/320946 [==============================] - 169s 527us/step - loss: 0.6859 - acc: 0.7743 - val_loss: 0.7676 - val_acc: 0.7513\n",
      "Epoch 7/10\n",
      "320946/320946 [==============================] - 163s 507us/step - loss: 0.6835 - acc: 0.7766 - val_loss: 0.7795 - val_acc: 0.7458\n",
      "Epoch 8/10\n",
      "320946/320946 [==============================] - 170s 528us/step - loss: 0.6804 - acc: 0.7788 - val_loss: 0.7846 - val_acc: 0.7497s -  - ETA: 5s - loss: 0.6799 - acc:  - ETA: 5s - loss: 0.67 - - ETA: 1\n",
      "Epoch 9/10\n",
      "320946/320946 [==============================] - 169s 526us/step - loss: 0.6777 - acc: 0.7801 - val_loss: 0.7901 - val_acc: 0.7477\n",
      "Epoch 10/10\n",
      "320946/320946 [==============================] - 169s 528us/step - loss: 0.6752 - acc: 0.7822 - val_loss: 0.7947 - val_acc: 0.7504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x195377bf588>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
